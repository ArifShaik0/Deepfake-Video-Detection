{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport os\n\ndef extract_frames_from_videos(video_folder, output_folder, fps):\n    \n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    # List all video files in the video_folder\n    video_files = [f for f in os.listdir(video_folder) if f.endswith(('.mp4', '.avi', '.mov'))]\n\n    for video_file in video_files:\n        video_path = os.path.join(video_folder, video_file)\n        video_output_folder = os.path.join(output_folder, os.path.splitext(video_file)[0])\n        \n        if not os.path.exists(video_output_folder):\n            os.makedirs(video_output_folder)\n\n        # Open the video file\n        cap = cv2.VideoCapture(video_path)\n        original_fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_interval = int(original_fps / fps)\n\n        count = 0\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            if count % frame_interval == 0:\n                frame_filename = os.path.join(video_output_folder, f\"frame_{count}.jpg\")\n                cv2.imwrite(frame_filename, frame)\n            \n            count += 1\n\n        cap.release()\n        print(f\"Frames extracted from {video_file} and saved to {video_output_folder}\")\n\n# Define directories for the dataset\ndataset_folder = '/kaggle/input/ff-first100/FF++100each'\nreal_videos_folder = os.path.join(dataset_folder, 'Real')\nfake_videos_folder = os.path.join(dataset_folder, 'Fake')\n\n# Define output directories\noutput_folder_real = './Frames/Real'\noutput_folder_fake = './Frames/Fake'\n\n# Extract frames from videos in both folders\nextract_frames_from_videos(real_videos_folder, output_folder_real, fps=2)\nextract_frames_from_videos(fake_videos_folder, output_folder_fake, fps=2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mtcnn import MTCNN\nimport cv2\nimport os\n\ndef detect_and_crop_faces(input_folder, output_folder):\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    detector = MTCNN()\n\n    # Iterate over each subfolder in the input folder (each subfolder contains frames for one video)\n    for video_folder in os.listdir(input_folder):\n        video_folder_path = os.path.join(input_folder, video_folder)\n        video_output_folder = os.path.join(output_folder, video_folder)\n\n        if not os.path.isdir(video_folder_path):\n            continue\n\n        if not os.path.exists(video_output_folder):\n            os.makedirs(video_output_folder)\n\n        # Iterate over all frames in the video subfolder\n        for frame_file in os.listdir(video_folder_path):\n            frame_path = os.path.join(video_folder_path, frame_file)\n            \n            # Read the frame\n            image = cv2.imread(frame_path)\n            if image is None:\n                print(f\"Warning: {frame_path} could not be loaded.\")\n                continue\n            \n            # Detect faces in the frame\n            faces = detector.detect_faces(image)\n            if not faces:\n                print(f\"No faces detected in {frame_path}.\")\n                continue\n            \n            # Process each detected face\n            for i, face in enumerate(faces):\n                x, y, width, height = face['box']\n                x, y = max(0, x), max(0, y)\n\n                # Validate cropping coordinates\n                if x + width > image.shape[1] or y + height > image.shape[0]:\n                    print(f\"Invalid cropping coordinates for face {i} in {frame_path}.\")\n                    continue\n\n                # Crop the face\n                cropped_face = image[y:y+height, x:x+width]\n                \n                # Save the cropped face image\n                cropped_face_filename = os.path.join(video_output_folder, f\"{os.path.splitext(frame_file)[0]}_face_{i}.jpg\")\n                cv2.imwrite(cropped_face_filename, cropped_face)\n                print(f\"Saved cropped face to {cropped_face_filename}\")\n\n# Example usage\ninput_folder_real = '/kaggle/input/first100vids/kaggle/working/Frames/Real'\noutput_folder_real = './cropped_faces/Real'\ninput_folder_fake = '/kaggle/input/first100vids/kaggle/working/Frames/Fake'\noutput_folder_fake = './cropped_faces/Fake'\n\n# Detect and crop faces in both Real and Fake frames\ndetect_and_crop_faces(input_folder_real, output_folder_real)\ndetect_and_crop_faces(input_folder_fake, output_folder_fake)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport os\n\ndef resize_cropped_faces(input_folder, output_folder, target_size):\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    # Iterate through each subfolder in the input folder (each subfolder contains cropped faces for one video)\n    for video_folder in os.listdir(input_folder):\n        video_folder_path = os.path.join(input_folder, video_folder)\n        video_output_folder = os.path.join(output_folder, video_folder)\n\n        if not os.path.isdir(video_folder_path):\n            continue\n\n        if not os.path.exists(video_output_folder):\n            os.makedirs(video_output_folder)\n\n        # Iterate over all cropped face images in the video subfolder\n        for face_file in os.listdir(video_folder_path):\n            face_path = os.path.join(video_folder_path, face_file)\n            \n            # Read the cropped face image\n            image = cv2.imread(face_path)\n            if image is None:\n                print(f\"Warning: {face_path} could not be loaded.\")\n                continue\n            \n            # Resize the image to the target size\n            resized_image = cv2.resize(image, target_size)\n            \n            # Save the resized image\n            resized_image_filename = os.path.join(video_output_folder, face_file)\n            cv2.imwrite(resized_image_filename, resized_image)\n            print(f\"Resized image saved to {resized_image_filename}\")\n\n# Example usage\ninput_folder_real = '/kaggle/working/cropped_faces/Real'\noutput_folder_real = './resized_faces/Real'\ninput_folder_fake = '/kaggle/working/cropped_faces/Fake'\noutput_folder_fake = './resized_faces/Fake'\n\n# Resize the cropped faces to 224x224 for both Real and Fake frames\nresize_cropped_faces(input_folder_real, output_folder_real, target_size=(224, 224))\nresize_cropped_faces(input_folder_fake, output_folder_fake, target_size=(224, 224))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nimport os\nimport numpy as np\n\n# Helper function to load images and labels from a folder\ndef load_images_from_folder(folder, target_size=(224, 224)):\n    image_paths = []\n    labels = []\n\n    for label in ['real', 'fake']:\n        label_folder = os.path.join(folder, label)\n        for file_name in os.listdir(label_folder):\n            file_path = os.path.join(label_folder, file_name)\n            image_paths.append(file_path)\n            labels.append(0 if label == 'real' else 1)  # Real -> 0, Fake -> 1\n\n    return image_paths, np.array(labels)\n\n# Load training and validation images\ntrain_folder = '/kaggle/input/preprocessed-faces/data/train'  # Replace with your train folder path\nval_folder = '/kaggle/input/preprocessed-faces/data/val'      # Replace with your validation folder path\n\ntrain_image_paths, train_labels = load_images_from_folder(train_folder)\nval_image_paths, val_labels = load_images_from_folder(val_folder)\n\n# Helper function to process images\ndef preprocess_image(image_path, target_size=(224, 224)):\n    img = load_img(image_path, target_size=target_size)\n    img_array = img_to_array(img)\n    img_array = img_array / 255.0  # Normalize to [0, 1]\n    return img_array\n\n# Helper function to apply augmentation (for training set)\ndef augment_image(image):\n    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.random_transform(image)\n\n# Custom data generator for batch processing\ndef custom_data_generator(image_paths, labels, batch_size, is_training=True):\n    while True:\n        indices = np.arange(len(image_paths))\n        if is_training:\n            np.random.shuffle(indices)  # Shuffle for training set\n\n        for start in range(0, len(image_paths), batch_size):\n            end = min(start + batch_size, len(image_paths))\n            batch_indices = indices[start:end]\n            batch_images = []\n            batch_labels = labels[batch_indices]\n\n            for i in batch_indices:\n                img_array = preprocess_image(image_paths[i])\n\n                if is_training:\n                    img_array = augment_image(img_array)\n\n                batch_images.append(img_array)\n\n            yield np.array(batch_images), batch_labels\n\n# Parameters\nbatch_size = 32\ntrain_steps = len(train_image_paths) // batch_size\nval_steps = len(val_image_paths) // batch_size\n\n# Create data generators\ntrain_generator = custom_data_generator(train_image_paths, train_labels, batch_size, is_training=True)\nval_generator = custom_data_generator(val_image_paths, val_labels, batch_size, is_training=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# Load the base ResNet50 model pre-trained on ImageNet\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze all layers in the base model initially\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add custom layers on top of ResNet\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(1, activation='sigmoid')(x)\n\n# Create the full model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Unfreeze the last few layers of the ResNet model for fine-tuning\n# for layer in base_model.layers[-10:]:  # Unfreeze last 10 layers for training\n#     layer.trainable = True\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('best_resnet.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n\n# Train the model with your own datasets\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_steps,\n    validation_data=val_generator,\n    validation_steps=val_steps,\n    epochs=30,\n    callbacks=[checkpoint, reduce_lr]\n)\n\nmodel.save('last_resnet.keras')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef print_and_plot_metrics(history):\n    # Extract metrics from the history object\n    epochs = range(1, len(history.history['accuracy']) + 1)\n    \n    # Print metrics\n    print(\"Training and Validation Metrics:\")\n    for key in history.history.keys():\n        print(f\"{key}: {history.history[key][-1]}\")\n\n    # Plot accuracy\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Accuracy over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history.history['loss'], label='Training Loss')\n    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n    plt.title('Loss over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.show()\n\n# Assuming you have trained the model and have the history object\nprint_and_plot_metrics(history)","metadata":{},"execution_count":null,"outputs":[]}]}